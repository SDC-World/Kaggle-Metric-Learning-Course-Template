# scripts/train.py

import argparse
import os
import yaml
import random
import numpy as np
import pandas as pd
import torch
from tqdm.auto import tqdm

# --- –í–ê–ñ–ù–û: –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ —ç—Ç–∏ –∫–ª–∞—Å—Å—ã –±—ã–ª–∏ –±—ã –≤ src/ ---
# –ù–æ –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã —Å—Ç–∞—Ä—Ç–∞ –≤ –ú–æ–¥—É–ª–µ 0 –º—ã –æ—Å—Ç–∞–≤–∏–º –∏—Ö –∑–¥–µ—Å—å –∫–∞–∫ –∑–∞–≥–ª—É—à–∫–∏.

def seed_everything(seed: int):
    """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç seed –¥–ª—è –≤—Å–µ—Ö –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ —Å–ª—É—á–∞–π–Ω—ã—Ö —á–∏—Å–µ–ª –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏."""
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

class DummyDataset(torch.utils.data.Dataset):
    """–ü—Ä–æ—Å—Ç–æ–π Dataset-–∑–∞–≥–ª—É—à–∫–∞, –∫–æ—Ç–æ—Ä—ã–π –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ."""
    def __init__(self, df, config):
        self.df = df
        self.config = config

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        # –í —Ä–µ–∞–ª—å–Ω–æ–º –∑–∞–¥–∞–Ω–∏–∏ –∑–¥–µ—Å—å –±—ã–ª–∞ –±—ã –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π/—Ç–µ–∫—Å—Ç–∞
        dummy_input = torch.randn(3, 224, 224) # –ü—Ä–∏–º–µ—Ä "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"
        label = 0 # –í —Ä–µ–∞–ª—å–Ω–æ–º –∑–∞–¥–∞–Ω–∏–∏ –∑–¥–µ—Å—å –±—ã–ª–∞ –±—ã –º–µ—Ç–∫–∞
        return dummy_input, torch.tensor(label, dtype=torch.long)

class DummyModel(torch.nn.Module):
    """–ü—Ä–æ—Å—Ç–∞—è –ú–æ–¥–µ–ª—å-–∑–∞–≥–ª—É—à–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è" –∏ –≤—ã–¥–∞–µ—Ç –ª–æ–≥–∏—Ç—ã."""
    def __init__(self, config):
        super().__init__()
        # –í —Ä–µ–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ –∑–¥–µ—Å—å –±—ã–ª–∞ –±—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç–∏–ø–∞ EfficientNet/BERT
        self.backbone = torch.nn.AdaptiveAvgPool2d((1,1))
        self.fc = torch.nn.Linear(3, 1) # –í—ã—Ö–æ–¥ –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏

    def forward(self, x):
        x = self.backbone(x).view(x.size(0), -1)
        return self.fc(x)

# --- –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞ ---

def main():
    # 1. –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
    parser = argparse.ArgumentParser(description="Train a model based on a YAML config file.")
    parser.add_argument('--config', required=True, help="Path to the YAML configuration file.")
    args = parser.parse_args()

    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ YAML —Ñ–∞–π–ª–∞
    print(f"Loading configuration from {args.config}")
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)

    # 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    seed_everything(config['general']['seed'])

    # 4. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞
    output_dir = os.path.join("outputs", config['general']['experiment_name'])
    os.makedirs(output_dir, exist_ok=True)
    print(f"Output will be saved to {output_dir}")

    # 5. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Ñ–æ–ª–¥–æ–≤
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—É—â–µ–Ω –ª–∏ —Å–∫—Ä–∏–ø—Ç –≤ —Ç–µ—Å—Ç–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
    if config['data'].get('is_dummy', False):
        print("üöÄ Running in DUMMY mode for a quick test.")
        # –°–æ–∑–¥–∞–µ–º —Ñ–µ–π–∫–æ–≤—ã–π DataFrame –ø—Ä—è–º–æ –≤ –ø–∞–º—è—Ç–∏
        num_dummy_samples = 100 # –ù–∞–ø—Ä–∏–º–µ—Ä, 100 —Å—ç–º–ø–ª–æ–≤
        df_folds = pd.DataFrame({
            'id': range(num_dummy_samples),
            'target': np.random.randint(0, 2, num_dummy_samples)
        })
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ñ–æ–ª–¥—ã "–Ω–∞ –ª–µ—Ç—É"
        df_folds['fold'] = np.arange(num_dummy_samples) % config['data']['n_splits']
        
        # –í Dummy-—Ä–µ–∂–∏–º–µ ID –¥–ª—è OOF-—Ñ–∞–π–ª–∞ –Ω–µ —Ç–∞–∫ –≤–∞–∂–µ–Ω
        global MATCHING_ID_COLUMN 
        MATCHING_ID_COLUMN = 'id'

    else:
        print("üöÄ Running in REAL mode.")
        # –í–ê–ñ–ù–û: –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç—Å—è, —á—Ç–æ —Ñ–∞–π–ª —Å —Ñ–æ–ª–¥–∞–º–∏ —É–∂–µ —Å–æ–∑–¥–∞–Ω!
        folds_path = os.path.join(config['data']['path'], f"folds_{config['general']['experiment_name']}.csv")
        if not os.path.exists(folds_path):
            print(f"‚ùå Error: Folds file not found at {folds_path}")
            print("Please run the fold creation script first (—Ä–µ–∞–ª–∏–∑—É–π—Ç–µ –µ–≥–æ –≤ src/data/folds.py).")
            return
        
        df_folds = pd.read_csv(folds_path)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É ID –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ –¥–ª—è OOF
        global MATCHING_ID_COLUMN
        MATCHING_ID_COLUMN = config['data']['matching_id_column'] # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–æ–±–∞–≤–∏–ª–∏ —ç—Ç–æ –≤ —Ä–µ–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥–∏

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–∞—Å—Å–∏–≤ –¥–ª—è OOF-–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    oof_predictions = np.zeros(len(df_folds))

    # 6. –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è –ø–æ —Ñ–æ–ª–¥–∞–º
    for fold in range(config['data']['n_splits']):
        print(f"\n========== FOLD {fold} / {config['data']['n_splits'] - 1} ==========")

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/validation
        train_df = df_folds[df_folds['fold'] != fold].reset_index(drop=True)
        valid_df = df_folds[df_folds['fold'] == fold].reset_index(drop=True)

        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä–æ–≤
        train_dataset = DummyDataset(train_df, config)
        valid_dataset = DummyDataset(valid_df, config)
        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config['train_params']['batch_size'], shuffle=True)
        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=config['train_params']['batch_size'], shuffle=False)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏, –ª–æ—Å—Å–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = DummyModel(config).to(device)
        criterion = torch.nn.BCEWithLogitsLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=config['train_params']['learning_rate'])

        best_valid_loss = float('inf')

        # –¶–∏–∫–ª –ø–æ —ç–ø–æ—Ö–∞–º
        for epoch in range(config['train_params']['epochs']):
            model.train()
            train_loss = 0
            
            # --- Training Phase ---
            for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{config['train_params']['epochs']} - Train"):
                inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)
                
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()

            # --- Validation Phase ---
            model.eval()
            valid_loss = 0
            fold_preds = []
            with torch.no_grad():
                for inputs, labels in tqdm(valid_loader, desc=f"Epoch {epoch+1}/{config['train_params']['epochs']} - Valid"):
                    inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    valid_loss += loss.item()
                    fold_preds.append(outputs.sigmoid().cpu().numpy())

            train_loss /= len(train_loader)
            valid_loss /= len(valid_loader)
            
            print(f"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Valid Loss = {valid_loss:.4f}")

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if valid_loss < best_valid_loss:
                best_valid_loss = valid_loss
                torch.save(model.state_dict(), os.path.join(output_dir, f"model_best_fold_{fold}.pth"))
                print(f"‚ú® Model saved with best validation loss: {best_valid_loss:.4f}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
                oof_predictions[valid_df.index] = np.concatenate(fold_preds).flatten()

    # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ OOF –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    oof_df = pd.DataFrame({
        'id': df_folds[MATCHING_ID_COLUMN], # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —ç—Ç–∞ –∫–æ–ª–æ–Ω–∫–∞ –µ—Å—Ç—å –≤ –∫–æ–Ω—Ñ–∏–≥–µ/–¥–∞–Ω–Ω—ã—Ö
        'prediction': oof_predictions
    })
    oof_path = os.path.join(output_dir, "oof_predictions.csv")
    oof_df.to_csv(oof_path, index=False)
    print(f"\n‚úÖ OOF predictions saved to {oof_path}")


if __name__ == "__main__":
    main()